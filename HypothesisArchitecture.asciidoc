# Introduction

The hypothesis engine is a service that enables the user to test hypotheses about solutions deployed using Openshift.io. If Openshift can be thought of as an enterprise kubernetes environment for cloud native solutions, then Openshift.io can be thought of as a cloud based tool set to plan build and deploy these solutions in that environment. 
 
This hypothesis testing capability is realized by making it easy for the user/developer to conduct experiments that collect platform ( i.e supporting toolsets and infrastructure), process (i.e software delivery process ) and product (i.e user built solutions) metrics in a controlled and isolated manner. Within the context of these experiments it is possible to then postulate about the collected information. These postulates can then be used to either confirm or refute one or more hypotheses about the experimental data.
 
An experiment is created and configured by the user to run for one or more specified time intervals. During these intervals, the experiment is considered active and data is collected using one or more measuring instruments.  An instrument is simply a mechanism associated with an experiment that controls the measuring activity of an already defined metric. Metric definitions can be shared between experiments, but instruments are specific to an experiment. An instrument is also calibrated to take into account any constraints or compensations relevant to the measuring activity over time. Two instruments could measure the same metric in different ways, depending on the needs of the experiment. The instrument calibration also includes aspects such as sampling frequency and metric schema validation.
 
The available set of metric definitions includes several built in definitions for the platform, process and product categories. For metric definitions that don't exist, but that may be of interest to that are user, it is possible for the user to create custom metric definitions and categories. These custom definitions can be shared or private. The ability to define and control the visibility of these custom metrics is an important part of the system. User defined metrics can participate in the data collecting mechanism in the same manner as any built in metric definition can, thus facilitating the complementary ideas of an observable platform and metric ecosystem.
 
As data is collected from an active experiment, it is possible to make one or more postulates, or hypotheses, about the gathered information. These postulates can be reduced to simple falsifiable assertions that are testable in a repeatable way. A hypotheses can be created and tested during or after an active experiment, yielding a comparable time series of the postulate truthfulness over time. By careful analysis, arrangement and display of this information, it now becomes possible to make data driven inferences with a higher degree of certainty and confidence.
 
The goal of this experimentation capability is to provide an easy way to capture experimental data and then also provide easy access to the insights gleaned from this data. The possible scope of the information not only applies to the system under test, or the custom solution that the user is developing, but also the underlying software delivery processes that produce it and the infrastructure that enables it. The end result is a more holistic view of a cloud product development. This information can greatly help to inform the decision making process for those in planning and strategic roles. Furthermore, it is a great help for a more technical audience by enabling the more low level system metric inquiry and inference, thus reducing guess work about the non obvious relationships and interactions between cloud native system components. From a forward looking perspective, responses to these insights there exists the possibility to initiate responses using artificial intelligence that can make for self-healing cloud native systems.  In short, this data-driven, 'measure everything' approach is the foundation and key enabler of hypothesis driven development.


# HDD Architecture

Distributed systems have their own set of challenges. Breaking down a big difficult problem into smaller easier problems still faces the non trivial task of making those smaller problems work together as an integrated unit at scale. Notwithstanding all of this, it remains still an overall good strategy to divide and conquer, but it is important to strive for simplicity while avoiding over-simplification. In order to do this it is necessary to define sensible subsystem boundaries that focus on avoiding dependencies and fulfill clear responsibilities. Well chosen and defined subsystem boundaries make it easier to both manage and incrementally deploy due to minimal coupling.
 
For the hypothesis engine, 3 such  primary autonomous subsystem boundaries have been identified. Communication between these subsystems is done through api endpoints that send/receive data via JSON payloads over http(s) transport .

image::https://github.com/mik-dass/fabric8-hypothesis-documentation/blob/walk_doc/Images/architecture.png[]

1 User Interface(s) : 
The user interface is the primary means to orchestrate experiments, define metrics and visualize or view collected experimental data via an api. One or more user interfaces can be used.
 
2 Supervisor/Scout Master : 
The supervisor/scout-master is focused on the collection, aggregation and storage of data obtained from scouts/sensors. The context of received information is determined by supervisor managed experiments. The supervisor becomes the heart and brains of the observable platform, exposed as a service through several api endpoints.

Due to the differences in functionality and scaling requirements, each of the 3 different api classifications has become candidates for autonomous micro-services and accordingly they have been maintained as part of 3 different repos. In summary the responsibilities are as follows:

* Expose an experiment management api. This api is the mechanism used to create and coordinate experiments. This coordination includes configuration, execution and lifecycle semantics of isolated experiments. Another management aspect is the user driven design of schemas that serve as contracts for measurement data transmitted to the supervisor. Experiments are the means to collect measurements provided by scouts deployed and distributed throughout the system.  This data collection activity is responsible for validating, cleansing, transforming, filtering, correlating, aggregating and potentially storing ( for non transient ) collected data at predetermined intervals in an isolated manner. Repo link : https://github.com/fabric8-hdd/fabric8-hypothesis-manager-api
* Expose a data retrieval api to make available experimental data. Repo link : https://github.com/fabric8-hdd/fabric8-hypothesis-data-api
* Expose a data collection api that serves as the central destination for data sent by observing scouts. Repo link : https://github.com/fabric8-hdd/fabric8-hypothesis-measurements-api

3 Scout(s)/Sensor(s) : 
The scout/sensor is focused on the retrieval/extraction/transformation of metric data from various metric data sources and the transmission of this metric data to the centrally located supervisor via the data collection api. How this data is retrieved is an implementation detail. This approach conforms to a typical loosely coupled agent pattern and offers a lot of flexibility because all that matters is the transmission of information to a known supervisor endpoint. In summary the responsibilities are as follows:
The scout(s)/sensor(s) observe entities of interest and sends/pushes  the metric information to the centralized supervisor using a compliant metric schema. This data is obtained by extracting data from existing or yet to be defined data sources such as tooling persistent stores, various system logs or run time inquiry.
Allows for multiple transport choices. The transport choices could possibly include store and forward semantics for metrics that require guaranteed delivery capabilities, but this increases complexity and creates the need for multiple transport channels, nevertheless, the capability to plug in this choice will be exposed together with useful defaults
Allows for implementation of scouts/sensor(s) using multiple languages and technologies in order to allow as many metric system participants as possible. 
